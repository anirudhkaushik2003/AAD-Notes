# Week 4, Lecture
## Basic Greedy Design
  ### Greedy choice Property
   - We must know the first step towards solving the problem
  ### Optimal substructure property
   - After taking the first step we must be able to model the remaining problem as a smaller version of the original problem so that induction follows
   - Following this we'll get the last step the same as the first step by induction

## Activity Selection Problem
  - We have a set of activities 
  ![equation](https://latex.codecogs.com/png.latex?%5Cbg_white%20S%20%3D%20%7Ba_1%2Ca_2%2C...%2Ca_n%7D)<br>
  - ai needs resource during period \[si,fi) where si = start time and fi = finish time
  - We have to select the largerst possible set of non-overlapping (mutually compatible) activities.
  ### Greedy Method
   #### Greedy choice property
   - Out of all the n activities which is guaranteed to be in the optimum solution
   #### Optimum substructure property
   - In this case, all the remaining activities that are compatible with the first choice
   - Thus the remaining activities can be selected in the same manner
   ### The greedy choice
   - The activity with the least finish time belongs to the optimum solution, let it be a0
   - Let A be a max size subset of mutually compatible activities from S
   - Arrange the activities in A in increasing order of finish time
   - Let ak be the first activity
   - if ak = a0 then done as a0 belongs to maximum size subset
   - Otherwise construct B = A - {ak}  U {a0} by replacing ak with a0
   - B is also a valid solution since activities in A are disjoint, as f0 <= fk (a0 has the least finish time) => a0 won't overlap with any other activity
   - Hence B has the same number of elements as A and activities in B are disjoint as proved above, thus B is also a valid solution set
   ### The optimum substructure
   - Solve for all activities compatible with a0
   ### Pseudo code
   ```javascript
   function Activity_selection(s,f,n)
   A <- {a1}
   i <- 1
   for m <- 2 to n
      do if sm >= fi
        then A <- A U {am}
         i <- m    --- ai is most recent addition to a 
   return A
   ```
   `Assume f1 <= f2 <= f3 ...`
   - In the above code, we arrange all the activities on the basis of finishing time
   - We choose the activity with the lowest finish time first and add it to the set A
   - Then we only add an activity to A if its starting time is greater than the finish time of the most recently added activity
   - Thus is one single pass we can obtain the optimal solution
   - Thus after adding a1, we check a2 then a3 and so on
   - This can be done iteratively with a single pass for loop or using recursion
   - We can start with f1 with the least finish time and keep on recursively adding activities based on the above conditions
   - Since the recursion is a tail recursion we can make it into an iteration
   - We have already proved that the greedy algorithm gives us the optimal solution
   - The complexity will be sorting + O(logn)

## Huffman code
  - What is the most economical way to write a given long string (over some alphabet) in binary?
  ### Naive Approach
  - consider an alphabet set {A,B,C,D} and let this set make uo a string of length 130 million characters
  - a = 70 mil, B = 3 mil, C = 20 mil, D - 37 mil
  - The naive encoding would be to use 2 bits to represent each character, 00 for A 01 for B 10 for C and 11 for D
  - The total size for encoding will be 230 million bits
  - Hence this approach is a little inefficient
  ### Variable length encoding
  - Variable length encoding invovlves assigning a variable number of bits to each character to encode them
  - In order to be able to properly decode the string we need to assign codewords such that no codeword is the prefix of another codeword
  - The above is known as the prefix tree property
  - eg - A = 0, B = 100, C = 101, D = 11
  - Thus the lotal size for encoding is now reduced to 213 million bits which is a 17% improvement from the naive encoding
  - The full binary tree representation of the above concepts is as follows:<br>
  ![image](https://user-images.githubusercontent.com/71220864/133291919-4da6e8cc-75f7-4840-86ef-9abe74e24600.png)<br><br>
  - Thus every encoding can be expressed as a full binary tree
  - Every internal node must have exactly 2 children otherwise the encoding will be longer than required and size will increase
  ### Precise problem statement
  - Given a character set and respective frequencies f1, f2, f3 ..., we want a tree whose leaves each correspond to a symbol and which minimises the overall length of the encoding.<br>
  ![equation](https://latex.codecogs.com/png.latex?%5Cbg_white%20cost%5C%20of%5C%20the%5C%20tree%20%3D%20%5Csum_%7Bi%3D1%7D%5E%7Bn%7Df_i.%28depth%5C%20of%5C%20i%5E%7Bth%7D%5C%20symbol%5C%20in%5C%20the%5C%20tree%29)<br>
  ### The Greedy Choice
  - What are the two symbols that are guaranteed to be siblings at the bottom of the tree?
  - As it turns out, the two symbols with the smallest frequencies must be at the bottom of the optimal tree as cheldren of the deepest internal node as they will have the maximum length encoding and hence overall cost of encoding will be reduced
  - Otherwise swapping the above two symbols with whatever symbol is at the bottom of the tree would improve the overall cost
  ### The optimum substructure
  - Define the frequency of an internal node to be the sum of the frequencies of its descendant leaves
  - The cost of the tree is the sum of frequencies of all internal nodes and leaves except the root
  - let f1 and f2 be the leaves with the minimum frequencies, we already proved that f2 and f1 must be the bottom most
  - Any tree in which f1 and f2 are sibling leaves has cost (f1+f2) plus the cost for a tree with (n-1) leaves of frequencies (f1+f2), f3, f4, ..., fn<br><br>
  ![image](https://user-images.githubusercontent.com/71220864/133294231-483f3802-207e-4209-92cb-ed4b44119821.png)<br><br>
  - Hence the rest of the tree can be constructed by assigning a new symbol to f1 and f2 combined and saying that the frequency of the new symbol is (f1+f2) and then solve again for the remaining tree
  - Thus we replace 2 symbols with frequency f1 and f2 and replace them with a single symbol with frequency f1+f2, hence the total symbols are reduced by 1
  - If we are able to identify the bottom most leave nodes (i.e. the greedy choice property which we defined above) then we can remove the two children and create a new problem which will be a smaller version of the original and say that the optimal substructure property holds.
 ### Huffman encoding solution procedure
 - Above we have proved that both The Greedy choice and optimal substructure both hold for the above problem
 - take the 2 least frequently occuring nodes, merge them as described above (create a new symbol with its frequency as their sum) and then recurse 
 - Since this is tail recursion, we can do this with iteration
 ### Pseudo code
   ```javascript
   function huffman(f)
   Input: An array f[1,2,...,n] of frequencies
   Output: An encoding tree with n leaves
   
   let H be a priority queue of integers, ordered by f
   for i = 1 to n: insert(H,i)
   for k = n+1 to 2n-1:
      i = deletemin(H), j= deletemin(H)  --- get the 2 nodes with the least frequency
      create a node number k with children i,j
      f[k] = f[i] + f[j] -- frequency of this node is sum of frequencies of i and j
      insert(H,k)
   ```
 
## Entropy 
Information and optimum-encoding
  - In the previous case of Huffman encoding, for a random variable we can say that the minimum number of bits required to express the information is _something_
  - The entropy of the random variable can be defined as the minimum number of bits required to express the random variable on average
  - This is also the uncertainity of the random variable
  - More compression = less random = More predictable
  - Suppose there are n possible outcomes with probability p1,p2,p3,...,pn
  - lets assume that these are exactly the frequencies observed and that each of the pi's is of the form 1/2^k (otherwise huffman code will not be applicable/ may not be the optimum way to represent the encoding)
  - We wish to find the number of bits needed to encode the sequence of m values (m instances of the random variable) drawn from the distribution<br>
  ![equation](https://latex.codecogs.com/png.latex?%5Cbg_white%20%5Csum_%7Bi%3D1%7D%5E%7Bn%7Dmp_ilog%28%5Cfrac%7B1%7D%7Bp_i%7D%29)<br>
  - Here mpi is the frequency and log(1/pi) is the depth of the corresponding leaf
  - Thus the average number of bits required to encode a single draw from the distribution is:<br>
  ![equation](https://latex.codecogs.com/png.latex?%5Cbg_white%20%5Csum_%7Bi%3D1%7D%5E%7Bn%7Dp_ilog%28%5Cfrac%7B1%7D%7Bp_i%7D%29)<br>
  - The above is the entropy of the system or a measure of the randomness of the system ( i found this similar to the class 11th definition and hence will be adding more reference material as a separate file on a later date for self reference)
Note: This concludes the notes for this lecture, there was only one lecture this week and hence this lecture has not been given a number, only the week has a number
 
